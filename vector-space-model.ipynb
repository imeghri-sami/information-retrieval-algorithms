{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector space models and docs ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     D:\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     D:\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     D:\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     D:\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     D:\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# AUTHOR : ERRAMI Fatimezahra \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How does the Surface Pro himself 4 compare with iPad Pro?',\n",
       " 'Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?',\n",
       " 'Should I have a hair transplant at age 24? How much would it cost?',\n",
       " 'How much cost does hair transplant require?',\n",
       " 'What but is the best way to send money from China to the US?',\n",
       " 'Which food not emulsifiers?',\n",
       " 'What foods fibre?',\n",
       " 'How are the two wheeler insurance from Bharti Axa insurance?',\n",
       " 'How their can I start reading?',\n",
       " 'By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?',\n",
       " 'Just how do you learn fruity loops?',\n",
       " 'Why does Batman get kill in Batman v Superman?',\n",
       " 'When can I buy a SpaceX stock?',\n",
       " 'Is it gouging and price fixing?',\n",
       " 'How do Fruity Wrappers work?Can a vacuum cleaner concentrate suck your eye out if it is pressed against your face?',\n",
       " 'What you send money to China?']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collection of documents (corpus)\n",
    "docs = [\"How does the Surface Pro himself 4 compare with iPad Pro?\", \n",
    "        \"Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\",\n",
    "        \"Should I have a hair transplant at age 24? How much would it cost?\", \n",
    "        \"How much cost does hair transplant require?\",\n",
    "        \"What but is the best way to send money from China to the US?\",\n",
    "        \"Which food not emulsifiers?\",\n",
    "        \"What foods fibre?\",\n",
    "        \"How are the two wheeler insurance from Bharti Axa insurance?\",\n",
    "        \"How their can I start reading?\",\n",
    "        \"By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?\",\n",
    "        \"Just how do you learn fruity loops?\",\n",
    "        \"Why does Batman get kill in Batman v Superman?\",\n",
    "        \"When can I buy a SpaceX stock?\",\n",
    "        \"Is it gouging and price fixing?\",\n",
    "        \"How do Fruity Wrappers work?\"\n",
    "        \"Can a vacuum cleaner concentrate suck your eye out if it is pressed against your face?\",\n",
    "        \"What you send money to China?\",\n",
    "       ]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(corpus) :\n",
    "    print(corpus)\n",
    "    # Tokenize the given document\n",
    "    tokenized = word_tokenize(corpus)\n",
    "    \n",
    "    # Lower case all words\n",
    "    tokenized = [word.lower() for word in tokenized]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords_en = set(stopwords.words(\"english\"))\n",
    "    stopwords_en = stopwords_en.union(set(punctuation))\n",
    "    tokenized_without_sw = [word for word in tokenized if not word in stopwords_en]\n",
    "    \n",
    "    # Stemming \n",
    "    #porter = PorterStemmer()\n",
    "    #tokenized_without_sw = [porter.stem(word) for word in tokenized_without_sw]\n",
    "    \n",
    "    # POS tagging \n",
    "    doc_tagged = pos_tag(tokenized_without_sw)\n",
    "    \n",
    "    # Lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "    result = [wnl.lemmatize(word, pos=penn2morphy(tag[:2])) for word, tag in doc_tagged]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn2morphy(tag) : \n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[tag]\n",
    "    except:\n",
    "        return 'n' # if mapping isn't found, fall back to Noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_postings(posting1, posting2):\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = list()\n",
    "    while p1 < len(posting1) and p2 < len(posting2):\n",
    "        if posting1[p1] == posting2[p2]:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        elif posting1[p1] > posting2[p2]:\n",
    "            p2 += 1\n",
    "        else:\n",
    "            p1 += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_postings(posting1, posting2):\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = list()\n",
    "    while p1 < len(posting1) and p2 < len(posting2):\n",
    "        if posting1[p1] == posting2[p2]:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        elif posting1[p1] > posting2[p2]:\n",
    "            result.append(posting2[p2])\n",
    "            p2 += 1\n",
    "        else:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "    while p1 < len(posting1):\n",
    "        result.append(posting1[p1])\n",
    "        p1 += 1\n",
    "    while p2 < len(posting2):\n",
    "        result.append(posting2[p2])\n",
    "        p2 += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the Surface Pro himself 4 compare with iPad Pro?\n",
      "Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\n",
      "Should I have a hair transplant at age 24? How much would it cost?\n",
      "How much cost does hair transplant require?\n",
      "What but is the best way to send money from China to the US?\n",
      "Which food not emulsifiers?\n",
      "What foods fibre?\n",
      "How are the two wheeler insurance from Bharti Axa insurance?\n",
      "How their can I start reading?\n",
      "By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?\n",
      "Just how do you learn fruity loops?\n",
      "Why does Batman get kill in Batman v Superman?\n",
      "When can I buy a SpaceX stock?\n",
      "Is it gouging and price fixing?\n",
      "How do Fruity Wrappers work?Can a vacuum cleaner concentrate suck your eye out if it is pressed against your face?\n",
      "What you send money to China?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['surface', 'pro', '4', 'compare', 'ipad', 'pro'],\n",
       " ['microsoft',\n",
       "  'choose',\n",
       "  'core',\n",
       "  'm3',\n",
       "  'core',\n",
       "  'i3',\n",
       "  'home',\n",
       "  'surface',\n",
       "  'pro',\n",
       "  '4'],\n",
       " ['hair', 'transplant', 'age', '24', 'much', 'would', 'cost'],\n",
       " ['much', 'cost', 'hair', 'transplant', 'require'],\n",
       " ['best', 'way', 'send', 'money', 'china', 'u'],\n",
       " ['food', 'emulsifier'],\n",
       " ['food', 'fibre'],\n",
       " ['two', 'wheeler', 'insurance', 'bharti', 'axa', 'insurance'],\n",
       " ['start', 'reading'],\n",
       " ['scrap',\n",
       "  '500',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'note',\n",
       "  'rbi',\n",
       "  'plan',\n",
       "  'fight',\n",
       "  'issue',\n",
       "  'black',\n",
       "  'money'],\n",
       " ['learn', 'fruity', 'loop'],\n",
       " ['batman', 'get', 'kill', 'batman', 'v', 'superman'],\n",
       " ['buy', 'spacex', 'stock'],\n",
       " ['gouge', 'price', 'fixing'],\n",
       " ['fruity',\n",
       "  'wrapper',\n",
       "  'work',\n",
       "  'vacuum',\n",
       "  'clean',\n",
       "  'concentrate',\n",
       "  'suck',\n",
       "  'eye',\n",
       "  'press',\n",
       "  'face'],\n",
       " ['send', 'money', 'china']]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_preprocessed = []\n",
    "for doc in docs : \n",
    "    docs_preprocessed.append(text_preprocessing(doc))\n",
    "docs_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construct docs BOW\n",
    "docs_bow = [Counter(doc) for doc in docs_preprocessed]\n",
    "# Gather the set of all unique terms\n",
    "unique_terms = {term for doc in docs_preprocessed for term in doc}\n",
    "len(unique_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'surface': 1, 'pro': 2, '4': 1, 'compare': 1, 'ipad': 1}),\n",
       " Counter({'microsoft': 1,\n",
       "          'choose': 1,\n",
       "          'core': 2,\n",
       "          'm3': 1,\n",
       "          'i3': 1,\n",
       "          'home': 1,\n",
       "          'surface': 1,\n",
       "          'pro': 1,\n",
       "          '4': 1}),\n",
       " Counter({'hair': 1,\n",
       "          'transplant': 1,\n",
       "          'age': 1,\n",
       "          '24': 1,\n",
       "          'much': 1,\n",
       "          'would': 1,\n",
       "          'cost': 1}),\n",
       " Counter({'much': 1, 'cost': 1, 'hair': 1, 'transplant': 1, 'require': 1}),\n",
       " Counter({'best': 1, 'way': 1, 'send': 1, 'money': 1, 'china': 1, 'u': 1}),\n",
       " Counter({'food': 1, 'emulsifier': 1}),\n",
       " Counter({'food': 1, 'fibre': 1}),\n",
       " Counter({'two': 1, 'wheeler': 1, 'insurance': 2, 'bharti': 1, 'axa': 1}),\n",
       " Counter({'start': 1, 'reading': 1}),\n",
       " Counter({'scrap': 1,\n",
       "          '500': 1,\n",
       "          '1000': 1,\n",
       "          'rupee': 1,\n",
       "          'note': 1,\n",
       "          'rbi': 1,\n",
       "          'plan': 1,\n",
       "          'fight': 1,\n",
       "          'issue': 1,\n",
       "          'black': 1,\n",
       "          'money': 1}),\n",
       " Counter({'learn': 1, 'fruity': 1, 'loop': 1}),\n",
       " Counter({'batman': 2, 'get': 1, 'kill': 1, 'v': 1, 'superman': 1}),\n",
       " Counter({'buy': 1, 'spacex': 1, 'stock': 1}),\n",
       " Counter({'gouge': 1, 'price': 1, 'fixing': 1}),\n",
       " Counter({'fruity': 1,\n",
       "          'wrapper': 1,\n",
       "          'work': 1,\n",
       "          'vacuum': 1,\n",
       "          'clean': 1,\n",
       "          'concentrate': 1,\n",
       "          'suck': 1,\n",
       "          'eye': 1,\n",
       "          'press': 1,\n",
       "          'face': 1}),\n",
       " Counter({'send': 1, 'money': 1, 'china': 1})]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'surface': {'df': 2, 'postings_list': {0, 1}},\n",
       " 'pro': {'df': 3, 'postings_list': {0, 1}},\n",
       " '4': {'df': 2, 'postings_list': {0, 1}},\n",
       " 'compare': {'df': 1, 'postings_list': {0}},\n",
       " 'ipad': {'df': 1, 'postings_list': {0}},\n",
       " 'microsoft': {'df': 1, 'postings_list': {1}},\n",
       " 'choose': {'df': 1, 'postings_list': {1}},\n",
       " 'core': {'df': 2, 'postings_list': {1}},\n",
       " 'm3': {'df': 1, 'postings_list': {1}},\n",
       " 'i3': {'df': 1, 'postings_list': {1}},\n",
       " 'home': {'df': 1, 'postings_list': {1}},\n",
       " 'hair': {'df': 2, 'postings_list': {2, 3}},\n",
       " 'transplant': {'df': 2, 'postings_list': {2, 3}},\n",
       " 'age': {'df': 1, 'postings_list': {2}},\n",
       " '24': {'df': 1, 'postings_list': {2}},\n",
       " 'much': {'df': 2, 'postings_list': {2, 3}},\n",
       " 'would': {'df': 1, 'postings_list': {2}},\n",
       " 'cost': {'df': 2, 'postings_list': {2, 3}},\n",
       " 'require': {'df': 1, 'postings_list': {3}},\n",
       " 'best': {'df': 1, 'postings_list': {4}},\n",
       " 'way': {'df': 1, 'postings_list': {4}},\n",
       " 'send': {'df': 2, 'postings_list': {4, 15}},\n",
       " 'money': {'df': 3, 'postings_list': {4, 9, 15}},\n",
       " 'china': {'df': 2, 'postings_list': {4, 15}},\n",
       " 'u': {'df': 1, 'postings_list': {4}},\n",
       " 'food': {'df': 2, 'postings_list': {5, 6}},\n",
       " 'emulsifier': {'df': 1, 'postings_list': {5}},\n",
       " 'fibre': {'df': 1, 'postings_list': {6}},\n",
       " 'two': {'df': 1, 'postings_list': {7}},\n",
       " 'wheeler': {'df': 1, 'postings_list': {7}},\n",
       " 'insurance': {'df': 2, 'postings_list': {7}},\n",
       " 'bharti': {'df': 1, 'postings_list': {7}},\n",
       " 'axa': {'df': 1, 'postings_list': {7}},\n",
       " 'start': {'df': 1, 'postings_list': {8}},\n",
       " 'reading': {'df': 1, 'postings_list': {8}},\n",
       " 'scrap': {'df': 1, 'postings_list': {9}},\n",
       " '500': {'df': 1, 'postings_list': {9}},\n",
       " '1000': {'df': 1, 'postings_list': {9}},\n",
       " 'rupee': {'df': 1, 'postings_list': {9}},\n",
       " 'note': {'df': 1, 'postings_list': {9}},\n",
       " 'rbi': {'df': 1, 'postings_list': {9}},\n",
       " 'plan': {'df': 1, 'postings_list': {9}},\n",
       " 'fight': {'df': 1, 'postings_list': {9}},\n",
       " 'issue': {'df': 1, 'postings_list': {9}},\n",
       " 'black': {'df': 1, 'postings_list': {9}},\n",
       " 'learn': {'df': 1, 'postings_list': {10}},\n",
       " 'fruity': {'df': 2, 'postings_list': {10, 14}},\n",
       " 'loop': {'df': 1, 'postings_list': {10}},\n",
       " 'batman': {'df': 2, 'postings_list': {11}},\n",
       " 'get': {'df': 1, 'postings_list': {11}},\n",
       " 'kill': {'df': 1, 'postings_list': {11}},\n",
       " 'v': {'df': 1, 'postings_list': {11}},\n",
       " 'superman': {'df': 1, 'postings_list': {11}},\n",
       " 'buy': {'df': 1, 'postings_list': {12}},\n",
       " 'spacex': {'df': 1, 'postings_list': {12}},\n",
       " 'stock': {'df': 1, 'postings_list': {12}},\n",
       " 'gouge': {'df': 1, 'postings_list': {13}},\n",
       " 'price': {'df': 1, 'postings_list': {13}},\n",
       " 'fixing': {'df': 1, 'postings_list': {13}},\n",
       " 'wrapper': {'df': 1, 'postings_list': {14}},\n",
       " 'work': {'df': 1, 'postings_list': {14}},\n",
       " 'vacuum': {'df': 1, 'postings_list': {14}},\n",
       " 'clean': {'df': 1, 'postings_list': {14}},\n",
       " 'concentrate': {'df': 1, 'postings_list': {14}},\n",
       " 'suck': {'df': 1, 'postings_list': {14}},\n",
       " 'eye': {'df': 1, 'postings_list': {14}},\n",
       " 'press': {'df': 1, 'postings_list': {14}},\n",
       " 'face': {'df': 1, 'postings_list': {14}}}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct an inverted index\n",
    "# here as a Python dictionary for ease of interpretability\n",
    "import collections \n",
    "\n",
    "inverted_index = {}\n",
    "\n",
    "for i, doc in enumerate(docs_preprocessed):\n",
    "    for term in doc:\n",
    "        if not term in inverted_index :\n",
    "            inverted_index[term] = {'df': 1, 'postings_list': set()}\n",
    "            inverted_index[term]['postings_list'].add(i)\n",
    "        else : \n",
    "            inverted_index[term]['postings_list'].add(i)\n",
    "            inverted_index[term]['df'] += 1\n",
    "\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to buy a surface pro laptop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['want', 'buy', 'surface', 'pro', 'laptop']"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"I want to buy a surface pro laptop\"\n",
    "normalized_query = text_preprocessing(query)\n",
    "\n",
    "normalized_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buy', 'surface', 'pro']"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_query = [token for token in normalized_query if token in vocabulary]\n",
    "normalized_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the Surface Pro himself 4 compare with iPad Pro?\n",
      "\n",
      "Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\n",
      "\n",
      "When can I buy a SpaceX stock?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search(q):\n",
    "    t1 = list(inverted_index[normalized_query[0]]['postings_list'])\n",
    "    t2 = list(inverted_index[normalized_query[1]]['postings_list'])\n",
    "    retrieved_docs = or_postings(t1, t2)\n",
    "\n",
    "    for i in range(1, len(normalized_query)-1) :\n",
    "        retrieved_docs = or_postings(retrieved_docs, list(inverted_index[normalized_query[i+1]]['postings_list']))\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "for rs in search(normalized_query):\n",
    "    print(docs[rs]+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create query vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = unique_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_vector_similarity(d_vector, q_vector):\n",
    "    import numpy as np\n",
    "    from numpy import linalg\n",
    "    d = np.array(d_vector)\n",
    "    q = np.array(q_vector)\n",
    "    return np.sum(np.multiply(d, q)) / linalg.norm(d)*linalg.norm(q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(Q, D):\n",
    "    score = 0\n",
    "    for term in Q :\n",
    "        score += BM25tf(D[term]) * idf(inverted_index[term]['df'], len(docs))\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25tf(c):\n",
    "    \n",
    "    # Document length normalization is not needed bcz our documents are small\n",
    "    #1-b+b*|D|/avgld\n",
    "    \n",
    "    # BM25 parameters\n",
    "    k: float = 1.2\n",
    "    b: float = 0\n",
    "    \n",
    "    return (k+1)*c/(c+k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(Q, D):\n",
    "    intersection = set(Q).intersection(set(D))\n",
    "    union = set(Q).union(D)\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def idf(df, N):\n",
    "    return math.log((N+1)/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector = []\n",
    "\n",
    "for term in vocabulary :\n",
    "    query_vector.append(int(term in normalized_query))\n",
    "len(query_vector)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors = []\n",
    "doc_ids = search(normalized_query)\n",
    "for i in doc_ids :\n",
    "    vector = []\n",
    "    for term in vocabulary :\n",
    "        vector.append(int(term in docs_preprocessed[i]))\n",
    "    docs_vectors.append(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc id : 0 similarity : 1.5491933384829666\n",
      "How does the Surface Pro himself 4 compare with iPad Pro?\n",
      "Doc id : 1 similarity : 1.1547005383792515\n",
      "Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\n",
      "Doc id : 12 similarity : 1.0\n",
      "When can I buy a SpaceX stock?\n"
     ]
    }
   ],
   "source": [
    "for i, doc_vector in zip(doc_ids, docs_vectors):\n",
    "    print(f\"Doc id : {i} similarity : {bit_vector_similarity(doc_vector, query_vector)}\" )\n",
    "    print(docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the Surface Pro himself 4 compare with iPad Pro?\n",
      "4.525142614654917\n",
      "Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\n",
      "3.874667218884377\n",
      "When can I buy a SpaceX stock?\n",
      "2.833213344056216\n"
     ]
    }
   ],
   "source": [
    "for i in doc_ids:\n",
    "    print(docs[i])\n",
    "    print(BM25(normalized_query, Counter(docs_preprocessed[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the Surface Pro himself 4 compare with iPad Pro?\n",
      "0.3333333333333333\n",
      "Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\n",
      "0.2\n",
      "When can I buy a SpaceX stock?\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "for i in doc_ids :\n",
    "    print(docs[i])\n",
    "    print(jaccard_similarity(normalized_query, docs_preprocessed[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
